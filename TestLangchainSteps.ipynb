{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading codebase from C:/Users/eswar/Downloads/chat-with-code-main...\n",
      "Success: Loaded 2 files with 9 chunks\n",
      "Loaded 2 files with 9 chunks\n"
     ]
    }
   ],
   "source": [
    "# In TestAPi.ipynb, add:\n",
    "from send_request import load_codebase\n",
    "\n",
    "# Load a directory of Python files\n",
    "load_codebase(\"C:/Users/eswar/Downloads/chat-with-code-main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asking codebase: What does this code do?\n",
      "\n",
      "Question:\n",
      "What does this code do?\n",
      "\n",
      "Answer:\n",
      "<think>\n",
      "Okay, I'm trying to figure out what this code does. Let me read through it carefully.\n",
      "\n",
      "So, first, the code imports several libraries like Streamlit and utils. It also sets a variable 'local' which is initially False. If it's True, it loads environment variables using load_dotenv from dotenv.\n",
      "\n",
      "Then there are multiple sections where a user can input an API key or link to a GitHub repo. Each time they enter an API key or repo, the code does some setup. They clone the repository and load the DB file, which probably means embedding the content into something like OpenAI's ChatGPT.\n",
      "\n",
      "Wait, but in the later part, I notice that each section is almost identical except for the order of imports and the local variable check. That might be a mistake because all these sections are doing the same thing. Maybe one of them was duplicated or there's an error here.\n",
      "\n",
      "Each time the user submits a repo or key, it updates the OpenAI API key in the environment variables and then clones the repository using utils.Embedder. The DB is loaded each time, which suggests that content is being embedded into the chat prompt.\n",
      "\n",
      "So putting this together, I think the code allows the user to enter their OpenAI API key or a GitHub link to a repo, clone that repo if it doesn't exist, and then use its contents for generating a chat response with someone. The specific repository used might be random or follow some order, but each run is independent because of the local variable check.\n",
      "\n",
      "I should also mention that without an actual content source, this code can't generate meaningful responses on its own. It's just embedding the content from the repo into the prompt.\n",
      "</think>\n",
      "\n",
      "The provided code allows users to input their OpenAI API key or a GitHub repository link and then clones the specified repository. The content of this clone is used to generate a chat response using OpenAI's AI model.\n",
      "\n",
      "ANSWER: This code enables users to use a cloned repository (from a given link) to generate chat responses by embedding its content into an API call, typically with OpenAI's model.\n",
      "\n",
      "Sources:\n",
      "- C:/Users/eswar/Downloads/chat-with-code-main\\chat-with-code-main\\chatbot.py\n"
     ]
    }
   ],
   "source": [
    "from send_request import ask_codebase\n",
    "\n",
    "# Ask about the loaded codebase\n",
    "ask_codebase(\"What does this code do?\", model=\"deepseek-r1:1.5b\", provider=\"ollama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asking codebase: I would like to know more about chatbot.py and its funtions what each funtion do\n",
      "\n",
      "Question:\n",
      "I would like to know more about chatbot.py and its funtions what each funtion do\n",
      "\n",
      "Answer:\n",
      "<think>\n",
      "Okay, I need to figure out what each function in chatbot.py does. Let me start by looking through the code provided.\n",
      "\n",
      "First, I see that the user has a title \"Chat-with-your-codebase\" and some functions are being called multiple times. The first part of the code seems to be initializing the app with OpenAI API keys. There's an if local check for dotenv loading, but it doesn't seem relevant here since all the code inside is repeated.\n",
      "\n",
      "Looking at the user inputs: there are two parts, one for the API key and another for the GitHub link. Both start by checking if a key is provided in the environment variables or if the user enters a text input. If either is true, the key is set, and then the repo is cloned from GitHub.\n",
      "\n",
      "Then, each time the code runs, it loads the repo again. The functions after that seem to be part of an error handling loop, where any errors are caught and logged. This suggests that if something goes wrong during processing, it's handled gracefully instead of crashing.\n",
      "\n",
      "Now, inside this function, there's a while True loop with two conditions: one checks if the bot is listening for messages, and the other checks if someone has entered a message before. If both are true, it starts chatting to the user.\n",
      "\n",
      "Looking at the functions:\n",
      "\n",
      "1. prompt() likely takes a user input and transforms it into a form that can be sent to the AI model.\n",
      "2. get_content() probably reads from the chat history or loads new content based on the prompt.\n",
      "3. generate_response() uses the model to generate the response based on the prompt received by prompt().\n",
      "4. stop_message() checks if the user has entered a specific message, which would prevent further interaction.\n",
      "\n",
      "I'm not entirely sure about every function's specifics without seeing their code, but I can see that they are responsible for handling the chat flow and processing messages.\n",
      "</think>\n",
      "\n",
      "The provided code is part of a Python application centered around a chatbot. Here's an explanation of each function:\n",
      "\n",
      "1. **prompt()**: This function converts user input into a format suitable for the AI model. It likely uses text processing to ensure the message is in the correct structure.\n",
      "\n",
      "2. **get_content()**: This function retrieves or loads content based on the prompt provided by `prompt()`.\n",
      "\n",
      "3. **generate_response()**: This function generates the response from the AI model, using the content processed by `get_content()` and executed by the `openai` library.\n",
      "\n",
      "4. **stop_message()**: This checks if a specific message has been entered by the user before starting to generate a response.\n",
      "\n",
      "5. **stop_chat()**: This stops the entire chat session when both messages are detected simultaneously, allowing for clean interaction.\n",
      "\n",
      "6. **start_chat()**: This initializes the chat bot, ensuring it's ready to handle user queries and start generating responses based on the content from GitHub repositories.\n",
      "\n",
      "7. **load_repo()**: Loads a specified Git repository URL, enabling access to an open-source project's codebase.\n",
      "\n",
      "8. **clone_repo()**: Clones a Git repository, ensuring the same content is accessible regardless of which version is used.\n",
      "\n",
      "Each function works together to facilitate the chatbot's ability to process and generate responses based on user queries, with the exception that all functions are repeated multiple times within the provided code context.\n",
      "\n",
      "Sources:\n",
      "- C:/Users/eswar/Downloads/chat-with-code-main\\chat-with-code-main\\chatbot.py\n"
     ]
    }
   ],
   "source": [
    "from send_request import ask_codebase\n",
    "\n",
    "# Ask about the loaded codebase\n",
    "ask_codebase(\"I would like to know more about chatbot.py and its funtions what each funtion do\", model=\"deepseek-r1:1.5b\", provider=\"ollama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asking codebase: What does this code do?\n",
      "Error: 500 - {\"detail\":\"400: No codebase loaded. Please load a codebase first.\"}\n"
     ]
    }
   ],
   "source": [
    "from send_request import ask_codebase\n",
    "\n",
    "# Ask about the loaded codebase\n",
    "ask_codebase(\"What does this code do?\", model=\"deepseek-r1:1.5b\", provider=\"ollama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asking codebase: I would like to know more about chatbot.py and its funtions what each funtion do\n",
      "\n",
      "Question:\n",
      "I would like to know more about chatbot.py and its funtions what each funtion do\n",
      "\n",
      "Answer:\n",
      "<think>\n",
      "Alright, I'm trying to understand what `chatbot.py` does and look into its functions. Let's see... The user provided a code snippet from streamlit as well, but it seems like the main focus is on `chatbot.py`. \n",
      "\n",
      "First, I'll check for any imports. Oh right, there are imports from langchain, utils, time, os, and dotenv. So that tells me we're dealing with some language chain models, utility functions, timing, file handling, environment variables, and logging.\n",
      "\n",
      "Looking at the code, the main function is `chatbot()`. Let's break it down step by step.\n",
      "\n",
      "1. **Initialization**: The function starts by initializing a chat history list called `session_state.messages` as empty. That makes sense because we'll be adding user messages to this list as they come in.\n",
      "\n",
      "2. **Handling User Input**: There's an `if local:` check again, which checks if the environment is local. If it is, the user enters their OpenAI API key, else they enter a text input. This sets up how user keys are handled.\n",
      "\n",
      "3. **Environment Variables and Models**: Inside the function, several variables are set. `LlamaCpp` is defined with specific parameters like model name, temperature, max tokens, and chunk size. Similarly, a Preg下的模型名称 and configuration are created. These look like some kind of chatbot model or language model setup.\n",
      "\n",
      "4. **Initializing Embeddings**: They create an OpenAI embeddings instance, set the response distance metric to 'cos' (cosine similarity), specify `k=3`, and fetch a keyword vector with `fetch_k=100`. This tells me they're using Llama for embedding the chat history into vectors.\n",
      "\n",
      "5. **Retrieving Questions**: They define a function called `retrieve_questions()` which takes a query as input. It uses langchain's text splitter to split the query into chunks, then uses LangChain's document_loaders and embeddings to get embeddings of each chunk. This seems like they're trying to handle multiple questions or long queries by splitting them up.\n",
      "\n",
      "6. **Creating Chain**: They create a Conversational RetrievalChain with 'stuff' type, which probably means it handles multiple steps for each response. The retriever is set to use the Llama embeddings with cosine similarity as the distance metric and 3 closest matches (k=3). This setup suggests they're using a chatbot model that can handle multi-step responses.\n",
      "\n",
      "7. **Processing Input**: When the user sends a question, it's passed into `retrieve_questions()`, which processes the query into chunks, retrieves embeddings for each part, and then calls the chain to get an answer. The result is added back to the queue with the assistant's response.\n",
      "\n",
      "8. **Adding Response**: After processing, they add the chat history message with the assistant's response using the queue. This means future users can see both the user's question and the bot's reply in the chat.\n",
      "\n",
      "I'm a bit confused about how exactly the embeddings are being used for retrieval. The function `retrieve_questions()` is called when a user sends a query, but where does that get added to? It seems like each chunk of the query is processed by the model, leading to multiple responses. But I need to understand if this is for multi-part questions or if it's handling single long queries.\n",
      "\n",
      "Also, why are embeddings being used here? Is this part of a larger system, maybe splitting a single question into multiple parts and getting each part handled separately? That might be useful for API calls that require multiple requests.\n",
      "\n",
      "I'm not entirely sure how the chat history is maintained across app reruns. It uses `st.session_state.messages` to store messages. When the user enters a new message after an assistant response, it's added back into this list, which should update the display correctly.\n",
      "\n",
      "Another thing I notice is that the code doesn't import everything from langchain directly; they're using functions and classes within the module. For example, `ChatOpenAI` is probably a class from the langchain.chat_models module, so accessing it as `ChatOpenAI(...)` makes sense in this context.\n",
      "\n",
      "I'm thinking about how to structure this explanation. I'll need to go through each function step by step, explaining what they do based on their code. Making sure to refer back to the imports and any variables defined earlier would help clarify things.\n",
      "</think>\n",
      "\n",
      "The provided code snippet outlines the `chatbot.py` module from the LangChain library. Here's a breakdown of its functions:\n",
      "\n",
      "1. **Initialization (`chatbot()` function)**:\n",
      "   - Initializes a chat history list to track user messages.\n",
      "\n",
      "2. **User Input Handling**:\n",
      "   - Accepts OpenAI API keys or text inputs for the OpenAI API key, setting up environment variables and model configurations.\n",
      "\n",
      "3. **Model Setup**:\n",
      "   - Configures and initializes Llama models with specific parameters (temperature, max tokens) and a Preg model configuration.\n",
      "\n",
      "4. **Embedding Creation**:\n",
      "   - Sets up an OpenAI embeddings instance with distance metric, chunk size, and keyword vector fetching settings.\n",
      "\n",
      "5. **Question Retrieval**:\n",
      "   - Processes user queries into chunks, retrieves embeddings using Llama, handles multiple responses, and creates a response chain for processing.\n",
      "\n",
      "6. **Response Handling**:\n",
      "   - Uses the created chain to retrieve and respond to questions, adding each response back to the chat history queue with the assistant's reply.\n",
      "\n",
      "7. **Message Management**:\n",
      "   - Maintains a session state for user messages using `st.session_state.messages`, allowing updates across app reruns.\n",
      "\n",
      "This module is designed to handle multi-step responses and split long queries into manageable parts, leveraging language models and embeddings within LangChain.\n",
      "\n",
      "Sources:\n",
      "- C:/Users/eswar/Downloads/chat-with-code-main\\chat-with-code-main\\utils.py\n",
      "- C:/Users/eswar/Downloads/chat-with-code-main\\chat-with-code-main\\chatbot.py\n"
     ]
    }
   ],
   "source": [
    "from send_request import ask_codebase\n",
    "\n",
    "# Ask about the loaded codebase\n",
    "ask_codebase(\"I would like to know more about chatbot.py and its funtions what each funtion do\", model=\"deepseek-r1:1.5b\", provider=\"ollama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eswar\\OneDrive\\Desktop\\Ai_Automation\\Ai_AutonmationBackend\\debug\\notebook_utils.py:13: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  self.vector_store = Chroma(persist_directory=chroma_db_path, embedding=self.embeddings)\n",
      "C:\\Users\\eswar\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\eswar\\OneDrive\\Desktop\\Ai_Automation\\Ai_AutonmationBackend\\debug\\notebook_utils.py:19: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  self.vector_store = Chroma(persist_directory=chroma_db_path, embedding_function=self.embeddings)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Could not import chromadb python package. Please install it with `pip install chromadb`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\langchain_community\\vectorstores\\chroma.py:83\u001b[39m, in \u001b[36mChroma.__init__\u001b[39m\u001b[34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'chromadb'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdebug\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvisualization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m plot_similarity_distribution, plot_retrieval_comparison, create_heatmap\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Initialize debug utils\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m debug_utils = \u001b[43mDebugUtils\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# 1. Visualize retrieval for a specific question\u001b[39;00m\n\u001b[32m     10\u001b[39m question = \u001b[33m\"\u001b[39m\u001b[33mWhat does the generate_response function do?\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eswar\\OneDrive\\Desktop\\Ai_Automation\\Ai_AutonmationBackend\\debug\\notebook_utils.py:19\u001b[39m, in \u001b[36mDebugUtils.__init__\u001b[39m\u001b[34m(self, chroma_db_path)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mself\u001b[39m.embeddings = HuggingFaceEmbeddings(\n\u001b[32m     14\u001b[39m     model_name=\u001b[33m\"\u001b[39m\u001b[33msentence-transformers/all-MiniLM-L6-v2\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     15\u001b[39m     model_kwargs={\u001b[33m\"\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m     16\u001b[39m )\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(chroma_db_path):\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     \u001b[38;5;28mself\u001b[39m.vector_store = \u001b[43mChroma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchroma_db_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mChroma DB not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchroma_db_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Please load a codebase first.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\langchain_core\\_api\\deprecation.py:214\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    213\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\langchain_community\\vectorstores\\chroma.py:86\u001b[39m, in \u001b[36mChroma.__init__\u001b[39m\u001b[34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn)\u001b[39m\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     87\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCould not import chromadb python package. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     88\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease install it with `pip install chromadb`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     89\u001b[39m     )\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     92\u001b[39m     \u001b[38;5;28mself\u001b[39m._client_settings = client_settings\n",
      "\u001b[31mImportError\u001b[39m: Could not import chromadb python package. Please install it with `pip install chromadb`."
     ]
    }
   ],
   "source": [
    "# Import debug utilities\n",
    "from debug.notebook_utils import DebugUtils\n",
    "from debug.debug_retrieval import debug_retrieval\n",
    "from debug.visualization import plot_similarity_distribution, plot_retrieval_comparison, create_heatmap\n",
    "\n",
    "# Initialize debug utils\n",
    "debug_utils = DebugUtils()\n",
    "\n",
    "# 1. Visualize retrieval for a specific question\n",
    "question = \"What does the generate_response function do?\"\n",
    "debug_utils.visualize_retrieval(question, k=5)\n",
    "\n",
    "# 2. Plot similarity scores for the retrieved chunks\n",
    "debug_utils.plot_similarity_scores(question, k=5)\n",
    "\n",
    "# 3. Try different retrieval parameters and compare\n",
    "results = [\n",
    "    {\"k\": 3, \"search_type\": \"similarity\", \"count\": 3},\n",
    "    {\"k\": 5, \"search_type\": \"similarity\", \"count\": 5},\n",
    "    {\"k\": 7, \"search_type\": \"similarity\", \"count\": 7}\n",
    "]\n",
    "plot_retrieval_comparison(question, results, metric=\"count\", title=\"Comparison of Retrieval Parameters\")\n",
    "\n",
    "# 4. For more detailed CLI-style output (will display in notebook output cell)\n",
    "from rich.console import Console\n",
    "console = Console()\n",
    "with console.capture() as capture:\n",
    "    debug_retrieval(question, k=5)\n",
    "output = capture.get()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced debugging: Compare different questions\n",
    "questions = [\n",
    "    \"What does the generate_response function do?\",\n",
    "    \"How does the code handle user input?\",\n",
    "    \"What is the main purpose of chatbot.py?\"\n",
    "]\n",
    "\n",
    "# Create a heatmap to visualize document relevance across questions\n",
    "import numpy as np\n",
    "\n",
    "# Get documents for each question\n",
    "all_docs = []\n",
    "all_sources = set()\n",
    "for q in questions:\n",
    "    retriever = debug_utils.vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "    docs = retriever.get_relevant_documents(q)\n",
    "    all_docs.append(docs)\n",
    "    for doc in docs:\n",
    "        all_sources.add(doc.metadata.get(\"source\", \"Unknown\"))\n",
    "\n",
    "# Create a matrix of document relevance\n",
    "sources_list = list(all_sources)\n",
    "matrix = np.zeros((len(questions), len(sources_list)))\n",
    "\n",
    "for i, docs in enumerate(all_docs):\n",
    "    for doc in docs:\n",
    "        source = doc.metadata.get(\"source\", \"Unknown\")\n",
    "        j = sources_list.index(source)\n",
    "        matrix[i, j] = 1  # Mark as relevant\n",
    "\n",
    "# Create the heatmap\n",
    "create_heatmap(\n",
    "    similarity_matrix=matrix,\n",
    "    x_labels=[os.path.basename(s) for s in sources_list],\n",
    "    y_labels=[f\"Q{i+1}: {q[:20]}...\" for i, q in enumerate(questions)],\n",
    "    title=\"Document Relevance Across Different Questions\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
